{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task Allotted\n",
    "\n",
    "## Data Science\n",
    "\n",
    "#### DS1: A company has a dataset that has long descriptions (Introduction) of itâ€™s products, Now as a DS Help the company to make a text summariser that takes these descriptions as input and summarises them into shorter versions without loosing the context. The length of the summary is also adjustable by the user. Also Document each stage of the code by adding proper comments. Mention the key technologies used and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Technologies Used are : Python packages like Natural Language ToolKit, Regex, Numpy, Pandas\n",
    "#### Length of Summary can be adjusted by changing the value of average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import math\n",
    "import operator\n",
    "import statistics\n",
    "from string import punctuation\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english') + list(punctuation))\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_string_special_characters(s):\n",
    "    \"\"\"\n",
    "    This function removes special characters from within a string.\n",
    "    parameters: \n",
    "        s(str): single input string.\n",
    "    return: \n",
    "        stripped(str): A string with special characters removed.\n",
    "    \"\"\"\n",
    "\n",
    "    # Replace special character with ' '\n",
    "    stripped = re.sub('[^\\w\\s]', '', s)\n",
    "    stripped = re.sub('_', '', stripped)\n",
    "\n",
    "    # Change any whitespace to one space\n",
    "    stripped = re.sub('\\s+', ' ', stripped)\n",
    "\n",
    "    # Remove start and end white spaces\n",
    "    stripped = stripped.strip()\n",
    "    \n",
    "    return stripped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    \"\"\"This function returns the \n",
    "    total number of words in the input text.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    words = word_tokenize(text)\n",
    "    for word in words:\n",
    "        count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc(text_sents_clean):\n",
    "    \"\"\"\n",
    "    this function splits the text into sentences and\n",
    "    considering each sentence as a document, calculates the \n",
    "    total word count of each.\n",
    "    \"\"\"\n",
    "    doc_info = []\n",
    "    i = 0\n",
    "    for sent in text_sents_clean:\n",
    "        i += 1 \n",
    "        count = count_words(sent)\n",
    "        temp = {'doc_id' : i, 'doc_length' : count}\n",
    "        doc_info.append(temp)\n",
    "    return doc_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_freq_dict(sents):\n",
    "    \"\"\"\n",
    "    This function creates a frequency dictionary\n",
    "    of each document that contains words other than\n",
    "    stop words.\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    freqDict_list = []\n",
    "    for sent in sents:\n",
    "        i += 1\n",
    "        freq_dict = {}\n",
    "        words = word_tokenize(sent)\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            word = ps.stem(word)\n",
    "            if word not in stop_words:\n",
    "                if word in freq_dict:\n",
    "                    freq_dict[word] += 1\n",
    "                else:\n",
    "                    freq_dict[word] = 1\n",
    "                temp = {'doc_id' : i, 'freq_dict': freq_dict}\n",
    "        freqDict_list.append(temp)\n",
    "    return freqDict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_frequency(text_sents_clean):\n",
    "    \"\"\"\n",
    "    This function returns a dictionary with the frequency \n",
    "    count of every word in the text\n",
    "    \"\"\"\n",
    "    freq_table = {}\n",
    "    text = ' '.join(text_sents_clean) #join the cleaned sentences to get the text \n",
    "    words = word_tokenize(text)\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        word = ps.stem(word)\n",
    "        if word not in stop_words:\n",
    "            if word in freq_table:\n",
    "                freq_table[word] += 1\n",
    "            else:\n",
    "                freq_table[word] = 1\n",
    "    return freq_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(text_sents_clean):\n",
    "    \"\"\"\n",
    "    This function gets the top 5 most\n",
    "    frequently occuring words in the whole text\n",
    "    and stores them as keywords\n",
    "    \"\"\"\n",
    "    freq_table = global_frequency(text_sents_clean)\n",
    "    #sort in descending order\n",
    "    freq_table_sorted = sorted(freq_table.items(), key = operator.itemgetter(1), reverse = True) \n",
    "    keywords = []\n",
    "    for i in range(0, 5):  #taking first 5 most frequent words\n",
    "        keywords.append(freq_table_sorted[i][0])\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(doc_info, freqDict_list):\n",
    "    \"\"\"\n",
    "    tf = (frequency of the term in the doc/total number of terms in the doc)\n",
    "    \"\"\"\n",
    "    TF_scores = []\n",
    "    \n",
    "    for tempDict in freqDict_list:\n",
    "        id = tempDict['doc_id']\n",
    "        for k in tempDict['freq_dict']:\n",
    "            temp = {'doc_id' : id,\n",
    "                    'TF_score' : tempDict['freq_dict'][k]/doc_info[id-1]['doc_length'],\n",
    "                   'key' : k}\n",
    "            TF_scores.append(temp)\n",
    "    return TF_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(doc_info, freqDict_list):\n",
    "    \"\"\"\n",
    "    idf = ln(total number of docs/number of docs with term in it)\n",
    "    \"\"\"\n",
    "    \n",
    "    IDF_scores = []\n",
    "    counter = 0\n",
    "    for dict in freqDict_list:\n",
    "        counter += 1\n",
    "        for k in dict['freq_dict'].keys():\n",
    "            count = sum([k in tempDict['freq_dict'] for tempDict in freqDict_list])\n",
    "            temp = {'doc_id' : counter, 'IDF_score' : math.log(len(doc_info)/count), 'key' : k}\n",
    "    \n",
    "            IDF_scores.append(temp)\n",
    "                \n",
    "    return IDF_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDF(TF_scores, IDF_scores):\n",
    "    \"\"\"\n",
    "    TFIDF is computed by multiplying the coressponding\n",
    "    TF and IDF values of each term. \n",
    "    \"\"\"\n",
    "    TFIDF_scores = []\n",
    "    for j in IDF_scores:\n",
    "        for i in TF_scores:\n",
    "            if j['key'] == i['key'] and j['doc_id'] == i['doc_id']:\n",
    "                temp = {'doc_id' : i['doc_id'],\n",
    "                        'TFIDF_score' : j['IDF_score']*i['TF_score'],\n",
    "                       'key' : i['key']}\n",
    "        TFIDF_scores.append(temp)\n",
    "    return TFIDF_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weigh_keywords(TFIDF_scores):\n",
    "    \"\"\"\n",
    "    This function doubles the TFIDF score\n",
    "    of the words that are keywords\n",
    "    \"\"\"\n",
    "    keywords = get_keywords(text_sents_clean)\n",
    "    for temp_dict in TFIDF_scores:\n",
    "        if temp_dict['key'] in keywords:\n",
    "            temp_dict['TFIDF_score'] *= 2\n",
    "    return TFIDF_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent_score(TFIDF_scores, text_sents, doc_info):\n",
    "    \"\"\"\n",
    "    This function prints out the summary and returns the \n",
    "    score of each sentence in a list.\n",
    "    \n",
    "    The score of a sentence is calculated by adding the TFIDF\n",
    "    scores of the words that make up the sentence.\n",
    "    \"\"\"\n",
    "    sentence_info = []\n",
    "    for doc in doc_info:\n",
    "        \"\"\"\n",
    "        This loops through each document(sentence)\n",
    "        and calculates their 'sent_score'\n",
    "        \"\"\"\n",
    "        sent_score = 0\n",
    "        for i in range(0, len(TFIDF_scores)):\n",
    "            temp_dict = TFIDF_scores[i]\n",
    "            if doc['doc_id'] == temp_dict['doc_id']:\n",
    "                sent_score += temp_dict['TFIDF_score']\n",
    "        temp = {'doc_id' : doc['doc_id'], 'sent_score' : sent_score,\n",
    "                'sentence' : text_sents[doc['doc_id']-1]}\n",
    "        sentence_info.append(temp)\n",
    "\n",
    "    return sentence_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(sentence_info):\n",
    "    sum = 0\n",
    "    summary = []\n",
    "    array = []\n",
    "    for temp_dict in sentence_info:\n",
    "        \"\"\"\n",
    "        This loop gets the sum of scores\n",
    "        of all the sentences.\n",
    "        \"\"\"\n",
    "        sum += temp_dict['sent_score']\n",
    "    avg = sum/len(sentence_info) #computing the average tf-idf score\n",
    "    for temp_dict in sentence_info:\n",
    "        \"\"\"\n",
    "        This loop gets the sentence scores \n",
    "        and stores them in an array.\n",
    "        \"\"\"\n",
    "        array.append(temp_dict['sent_score'])\n",
    "    stdev = statistics.stdev(array) #computing standard deviation on the array   \n",
    "    for sent in sentence_info:\n",
    "        \"\"\"\n",
    "        This loop is for getting the sumamry by \n",
    "        extracting sentences by an if clause\n",
    "        \"\"\"\n",
    "        if(sent['sent_score']) >= avg *1.1  : # Change the value of the constant multiplier, increase for less summary, descrease for more\n",
    "            summary.append(sent['sentence'])\n",
    "    summary = '\\n'.join(summary)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Task Data and finding the Summary for the given description and appending it to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('TASK.xlsx')\n",
    "intro = df['Unnamed: 1'][1:]\n",
    "idx = 0 \n",
    "summary_lst = []\n",
    "for text in intro:\n",
    "    if( len(text) < 100 ):\n",
    "        summary_lst.append(text)   # IF lenth of any description is < 100 then directly add it as is to the summary list.\n",
    "    else:\n",
    "        text_sents = sent_tokenize(text)\n",
    "        text_sents_clean = [remove_string_special_characters(s) for s in text_sents]\n",
    "        doc_info = get_doc(text_sents_clean)\n",
    "\n",
    "\n",
    "        freqDict_list = create_freq_dict(text_sents_clean)\n",
    "        TF_scores = computeTF(doc_info, freqDict_list)\n",
    "        IDF_scores = computeIDF(doc_info, freqDict_list)\n",
    "\n",
    "\n",
    "        TFIDF_scores = computeTFIDF(TF_scores, IDF_scores)\n",
    "        TFIDF_scores = weigh_keywords(TFIDF_scores)\n",
    "        sentence_info = get_sent_score(TFIDF_scores, text_sents, doc_info)\n",
    "        summary = get_summary(sentence_info)\n",
    "        summary_lst.append(summary)  # Appending description and summary together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the Summary obtained to a new file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro.index = np.arange(len(intro))\n",
    "summ = pd.DataFrame(summary_lst)\n",
    "intro = intro.rename(columns = {'Unnamed: 1':1})\n",
    "data = pd.concat((intro, summ), axis = 1)\n",
    "output = pd.DataFrame(data)\n",
    "output.columns = ['Description', 'Summary']\n",
    "output.to_excel('Output.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
